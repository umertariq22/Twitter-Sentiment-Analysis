Twitter Sentiment Analysis üìäProject OverviewThis project focuses on performing sentiment analysis on Twitter data. It involves web scraping tweets üê¶, preprocessing the text data, extracting features, and then training and evaluating machine learning models (specifically an Artificial Neural Network and Support Vector Machine) to classify the sentiment of tweets as positive, neutral, or negative.Table of ContentsProject OverviewFeaturesMethodologyResultsInstallationUsageRequirementsFeatures ‚ú®Web Scraping: Collects tweets from Twitter using Selenium. üï∏Ô∏èData Preprocessing: Cleans and prepares tweet data, including:Removing punctuationConverting text to lowercaseRemoving stopwordsStemming words using Porter StemmerHandling missing valuesSentiment Labeling: Assigns sentiment labels (positive, neutral, negative) to tweets using the VADER (Valence Aware Dictionary and sEntiment Reasoner) lexicon. üòäüòêüò†Feature Extraction: Transforms text data into numerical features using:Bigrams (CountVectorizer)TF-IDF (Term Frequency-Inverse Document Frequency)Machine Learning Models:Artificial Neural Network (ANN) üß†Support Vector Machine (SVM)Model Evaluation: Assesses model performance using metrics such as accuracy, precision, recall, F1-score, and confusion matrices. üìàVisualization: Generates word clouds for positive and negative tweets. ‚òÅÔ∏èMethodology üß™Web Scraping:Selenium is used to automate web browser interactions to log in to Twitter and scrape tweet content.Tweets are continuously scraped by scrolling down the page until a sufficient number of tweets are collected (e.g., over 1100).Scraped tweets are saved to a CSV file named tweets.csv.Preprocessing and Wrangling:The tweets.csv file is loaded into a Pandas DataFrame.The VADER sentiment intensity analyzer from nltk.sentiment.vader is used to assign a sentiment label (0: Negative, 1: Neutral, 2: Positive) based on the compound score.Missing labels are dropped.A preprocess function is applied to clean the tweet text:Removes newlines and splits sentences into words.Removes punctuation.Converts words to lowercase.Filters out URLs.Removes English stopwords.Applies Porter Stemming.Feature Extraction and Splitting:The preprocessed tweets are converted back into strings.CountVectorizer is used to create bigram features (X_BI).TfidfVectorizer is used to create TF-IDF features (X_TFIDF).The data is split into training and testing sets (80% train, 20% test) for both bigram and TF-IDF features.Sentiment labels are one-hot encoded for the ANN model.Modelling:Artificial Neural Network (ANN):A sequential Keras model is built with multiple dense layers and ReLU activation, ending with a softmax output layer for 3 classes.The model is compiled with Adam optimizer and categorical_crossentropy loss.Trained for 10 epochs with a batch size of 32.Evaluated separately on bigram and TF-IDF features.Support Vector Machine (SVM):An SVC classifier is used.Trained and evaluated separately on bigram and TF-IDF features.Results üìäThe project evaluates the performance of ANN and SVM models using both Bigram and TF-IDF feature extraction methods.ANN Model PerformanceBigrams:Accuracy: Approximately 73-74% on the test set.Loss: Decreases significantly during training, indicating good learning.Precision, Recall, F1-score:Negative (0): Precision: 0.73, Recall: 0.52, F1-score: 0.61Neutral (1): Precision: 0.68, Recall: 0.81, F1-score: 0.74Positive (2): Precision: 0.87, Recall: 0.77, F1-score: 0.82Confusion Matrix (Bigrams):[[ 38   9   5]
 [ 11 113  15]
 [  2  21  74]]
TF-IDF:Accuracy: Approximately 73-74% on the test set.Loss: Similar training behavior to Bigrams.Precision, Recall, F1-score:Negative (0): Precision: 0.73, Recall: 0.52, F1-score: 0.61Neutral (1): Precision: 0.68, Recall: 0.81, F1-score: 0.74Positive (2): Precision: 0.87, Recall: 0.77, F1-score: 0.82Confusion Matrix (TF-IDF):[[ 38   9   5]
 [ 11 113  15]
 [  2  21  74]]
SVM Model PerformanceBigrams:Accuracy: 0.7365Precision, Recall, F1-score:Negative (0): Precision: 0.73, Recall: 0.52, F1-score: 0.61Neutral (1): Precision: 0.68, Recall: 0.81, F1-score: 0.74Positive (2): Precision: 0.87, Recall: 0.77, F1-score: 0.82Confusion Matrix (Bigrams):[[ 38   9   5]
 [ 11 113  15]
 [  2  21  74]]
TF-IDF:Accuracy: 0.7365Precision, Recall, F1-score:Negative (0): Precision: 0.73, Recall: 0.52, F1-score: 0.61Neutral (1): Precision: 0.68, Recall: 0.81, F1-score: 0.74Positive (2): Precision: 0.87, Recall: 0.77, F1-score: 0.82Confusion Matrix (TF-IDF):[[ 38   9   5]
 [ 11 113  15]
 [  2  21  74]]
Visualizations üñºÔ∏èSentiment Distribution: A pie chart shows the distribution of sentiment labels (Negative, Neutral, Positive) in the dataset.Positive Word Cloud: A word cloud visualizes the most frequent words in positive tweets.Negative Word Cloud: A word cloud visualizes the most frequent words in negative tweets.Installation üíªClone the repository:git clone <repository_url>
cd twitter-sentiment-analysis
Create a virtual environment (recommended):python -m venv venv
source venv/bin/activate  # On Windows: `venv\Scripts\activate`
Install the required packages:pip install -r requirements.txt
Download NLTK data:The script automatically attempts to download stopwords, wordnet, and vader_lexicon. If there are issues, you might need to run these manually in a Python interpreter:import nltk
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('vader_lexicon')
Usage ‚ñ∂Ô∏èConfigure Selenium:Download the appropriate ChromeDriver for your Chrome browser version.Update the driver_path variable in the Jupyter notebook with the path to your ChromeDriver executable.Provide your Twitter email in the email variable.The script will prompt you for your Twitter password during execution.Run the Jupyter Notebook:jupyter notebook Twitter_Sentiment_Analysis.ipynb
Execute the cells sequentially. The web scraping part will open a Chrome browser, log in to Twitter, and start scraping tweets.Analyze Results:The notebook will output model performance metrics, classification reports, confusion matrices, and visualizations.Requirements üì¶The project requires the following Python libraries:numpy
